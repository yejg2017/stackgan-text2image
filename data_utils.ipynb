{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ye/user/yejg/SW_DATA/Or/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import tensorlayer as tl\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset='102flowers'\n",
    "need_256=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cwd=os.getcwd()\n",
    "img_dir=os.path.join(cwd,dataset)\n",
    "caption_dir=os.path.join(cwd,'cvpr2016_flowers/','text_c10')\n",
    "VOC_FIR=cwd+'/vocab.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  load captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ye/user/yejg/LEARN/DL_MODEL_LEARN/TensorFlow/GAN/stack-text-to-image/cvpr2016_flowers/text_c10 \n",
      " ['/home/ye/user/yejg/LEARN/DL_MODEL_LEARN/TensorFlow/GAN/stack-text-to-image/cvpr2016_flowers/text_c10/class_00026', '/home/ye/user/yejg/LEARN/DL_MODEL_LEARN/TensorFlow/GAN/stack-text-to-image/cvpr2016_flowers/text_c10/class_00039', '/home/ye/user/yejg/LEARN/DL_MODEL_LEARN/TensorFlow/GAN/stack-text-to-image/cvpr2016_flowers/text_c10/class_00049', '/home/ye/user/yejg/LEARN/DL_MODEL_LEARN/TensorFlow/GAN/stack-text-to-image/cvpr2016_flowers/text_c10/class_00083']\n"
     ]
    }
   ],
   "source": [
    "caption_sub_dir=load_folder_list(caption_dir)\n",
    "caption_dict={}\n",
    "processed_capts=[]\n",
    "print(caption_dir,'\\n',caption_sub_dir[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match file list = ['image_06519.txt', 'image_06491.txt', 'image_06494.txt', 'image_06521.txt', 'image_06500.txt', 'image_06488.txt', 'image_06508.txt', 'image_06513.txt', 'image_06507.txt', 'image_06515.txt', 'image_06495.txt', 'image_06514.txt', 'image_06506.txt', 'image_06522.txt', 'image_06489.txt', 'image_06527.txt', 'image_06504.txt', 'image_06490.txt', 'image_06525.txt', 'image_06496.txt', 'image_06497.txt', 'image_06516.txt', 'image_06499.txt', 'image_06526.txt', 'image_06524.txt', 'image_06503.txt', 'image_06498.txt', 'image_06517.txt', 'image_06493.txt', 'image_06509.txt', 'image_06510.txt', 'image_06505.txt', 'image_06523.txt', 'image_06512.txt', 'image_06487.txt', 'image_06501.txt', 'image_06502.txt', 'image_06520.txt', 'image_06518.txt', 'image_06492.txt', 'image_06511.txt']\n",
      "Number of files = 41\n"
     ]
    }
   ],
   "source": [
    "files=tl.files.load_file_list(path=caption_sub_dir[1],regx='^image_[0-9]+\\.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'captions_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-219cc2f9e04a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mcaption_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" * %d x %d captions found \"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'captions_dict' is not defined"
     ]
    }
   ],
   "source": [
    "for sub_dir in caption_sub_dir:\n",
    "    with tl.ops.suppress_stdout():\n",
    "        files=tl.files.load_file_list(path=sub_dir,regx='^image_[0-9]+\\.txt')\n",
    "        for i,f in enumerate(files):\n",
    "            file_dir=os.path.join(sub_dir,f)\n",
    "            key=int(re.findall('\\d+',f)[0])\n",
    "            t=open(file_dir,'r')\n",
    "            lines=[]\n",
    "            for line in t:\n",
    "                line=preprocess_caption(line)\n",
    "                lines.append(line)\n",
    "                processed_capts.append(tl.nlp.process_sentence(line,start_word='<S>',end_word='</S>'))\n",
    "            assert len(lines)==10\n",
    "            caption_dict[key]=lines\n",
    "print(\" * %d x %d captions found \" % (len(caption_dict), len(lines)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ye/user/yejg/SW_DATA/Or/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os.path\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "import scipy.misc\n",
    "import math\n",
    "import sys\n",
    "\n",
    "MODEL_DIR = './imagenet'\n",
    "DATA_URL = 'http://download.tensorflow.org/models/image/imagenet/inception-2015-12-05.tgz'\n",
    "softmax = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inception_score(images, splits=10):\n",
    "  assert(type(images) == list)\n",
    "  assert(type(images[0]) == np.ndarray)\n",
    "  assert(len(images[0].shape) == 3)\n",
    "  assert(np.max(images[0]) > 10)\n",
    "  assert(np.min(images[0]) >= 0.0)\n",
    "  inps = []\n",
    "  for img in images:\n",
    "    img = img.astype(np.float32)\n",
    "    inps.append(np.expand_dims(img, 0))\n",
    "  bs = 100\n",
    "  with tf.Session() as sess:\n",
    "    preds = []\n",
    "    n_batches = int(math.ceil(float(len(inps)) / float(bs)))\n",
    "    for i in range(n_batches):\n",
    "        sys.stdout.write(\".\")\n",
    "        sys.stdout.flush()\n",
    "        inp = inps[(i * bs):min((i + 1) * bs, len(inps))]\n",
    "        inp = np.concatenate(inp, 0)\n",
    "        pred = sess.run(softmax, {'ExpandDims:0': inp})\n",
    "        preds.append(pred)\n",
    "    preds = np.concatenate(preds, 0)\n",
    "    scores = []\n",
    "    for i in range(splits):\n",
    "      part = preds[(i * preds.shape[0] // splits):((i + 1) * preds.shape[0] // splits), :]\n",
    "      kl = part * (np.log(part) - np.log(np.expand_dims(np.mean(part, 0), 0)))\n",
    "      kl = np.mean(np.sum(kl, 1))\n",
    "      scores.append(np.exp(kl))\n",
    "    return np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.makedirs(MODEL_DIR)\n",
    "filename = DATA_URL.split('/')[-1]\n",
    "filepath = os.path.join(MODEL_DIR, filename)\n",
    "\n",
    "with tf.gfile.FastGFile(os.path.join(\n",
    "      MODEL_DIR, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"transpose_3:0\", shape=(1008, 2048), dtype=float32)\n",
      "Tensor(\"pool_3:0\", shape=(1, 1, 1, 2048), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Works with an arbitrary minibatch size.\n",
    "with tf.Session() as sess:\n",
    "    pool3 = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "    ops = pool3.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            shape = [s.value for s in shape]\n",
    "            new_shape = []\n",
    "            for j, s in enumerate(shape):\n",
    "                if s == 1 and j == 0:\n",
    "                    new_shape.append(None)\n",
    "                else:\n",
    "                    new_shape.append(s)\n",
    "            #o._shape = tf.TensorShape(new_shape)\n",
    "            o.set_shape=tf.TensorShape(new_shape)\n",
    "    w = sess.graph.get_operation_by_name(\"softmax/logits/MatMul\").inputs[1]\n",
    "    print(tf.transpose(w))\n",
    "    print(pool3)\n",
    "    logits = tf.matmul(tf.transpose(w),tf.expand_dims(tf.squeeze(pool3),1))\n",
    "    prob = tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"ExpandDims_1:0\", shape=(2048, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.expand_dims(tf.squeeze(pool3),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d7502d2df568>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'logits' is not defined"
     ]
    }
   ],
   "source": [
    "tf.nn.softmax(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
